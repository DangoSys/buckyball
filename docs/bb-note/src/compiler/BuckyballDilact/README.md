# Tile Dialect Refactoring Documentation

## Refactoring Background and Goals

The core goal of this refactoring is to introduce a new intermediate layer between Linalg Dialect and Buckyball Dialect - the Tile Dialect - to achieve clearer separation of responsibilities and better code organization. In the original architecture, the conversion from `linalg.matmul` to hardware instructions was completed in one step through `convert-linalg-to-buckyball`, which caused Buckyball Dialect to handle both the slicing logic for arbitrary-size matrices and hardware-level memory management and computation scheduling, resulting in overly mixed responsibilities. The new architecture splits the conversion process into two phases: `convert-linalg-to-tile` and `convert-tile-to-buckyball`, making each layer have a clear and single responsibility.

## New Architecture Design

The entire compilation flow is now divided into three clear layers. First is the Linalg layer, which represents high-level linear algebra operations, such as `linalg.matmul` representing matrix multiplication of arbitrary size. This layer does not care about hardware constraints. Next is the newly introduced Tile layer, whose core responsibility is to tile arbitrary-size matrix operations into fixed-size blocks that conform to hardware constraints. The Tile layer expresses this high-level tiling intent through the `tile.tile_matmul` operation. The specific tiling strategy, loop generation, and boundary handling are all implemented in the `convert-tile-to-buckyball` pass. Finally, the Buckyball layer focuses on hardware-level operations. `buckyball.bb_matmul` receives pre-tiled fixed-size matrix blocks and is responsible for generating precise hardware instruction sequences, including data movement (mvin/mvout), computation scheduling (mul_warp16), and memory address calculation.

## Tile Dialect Design Details

The Tile Dialect defines the `TileMatMulOp` operation, which accepts three memref parameters representing matrices A, B, and C respectively. The semantics of this operation are: perform multiplication on input matrices of arbitrary size, automatically handling tiling, padding, and loops. In implementation, `TileMatMulOp` will be converted by the `convert-tile-to-buckyball` pass into multiple `buckyball.bb_matmul` operations and corresponding `memref.subview` operations. This conversion process will consider hardware scratchpad size limitations, warp and lane parallelism constraints, and generate an optimal tiling strategy. The design philosophy of the Tile layer is to provide a platform-independent intermediate representation, allowing upper-layer optimizations to transform matrix operations without understanding specific hardware details.

## Buckyball Dialect Simplification

In the new architecture, the Buckyball Dialect has been significantly simplified. The original four operations `VecTileMatMulOp`, `MergeTileMatMulOp`, `MetaTileMatMulOp`, and `VecMulWarp16Op` have been unified into a single `MatMulOp`. This simplification is reasonable because the tiling logic has been moved up to the Tile layer, and the Buckyball layer only needs to express the single concept of "performing hardware-level multiplication on a matrix block that already conforms to hardware constraints." The lowering process of `buckyball.bb_matmul` will directly generate LLVM intrinsics: first load matrices A and B into the scratchpad through `Mvin_IntrOp`, then generate multiple `Mul_Warp16_IntrOp` operations based on warp and lane parameters for computation, and finally write the results back to main memory through `Mvout_IntrOp`. All address calculations and encodings are completed in this lowering process.

## Key Implementation Details

When implementing the `convert-linalg-to-tile` pass, the core logic is very simple: match the `linalg.matmul` operation and directly replace it with `tile.tile_matmul`, passing the same three memref operands. The role of this pass is mainly type and semantic conversion, indicating that we have moved from the general linear algebra operation domain into the hardware-oriented tile operation domain.

The `convert-tile-to-buckyball` pass is the most complex part of the entire refactoring. It needs to extract matrix dimension information (M, K, N) from the operands of `tile.tile_matmul`, then calculate the optimal tiling strategy based on hardware parameters (dim, warp, lane). For the K dimension, it will tile according to warp size; for M and N dimensions, it will consider scratchpad capacity limitations. Each tile corresponds to a `buckyball.bb_matmul` operation, and tiles are connected through `memref.subview` to create matrix views. Special attention should be paid to handling boundary cases: when matrix dimensions cannot be evenly divided by tile size, the actual size of the last tile needs to be calculated to avoid out-of-bounds access.

When implementing `BuckyballMatMulLowering`, we encountered an important concept in MLIR's type conversion system: OpAdaptor. In conversion patterns, the types of the original operation (such as `memref<32x16xi8>`) will be converted to LLVM types (such as LLVM struct types) by the TypeConverter during the lowering process. OpAdaptor provides converted values, but we need to obtain type information (such as shape) from the original operation because this static information may no longer exist in the same form after conversion. Therefore, the correct approach is: obtain the original `MemRefType` from `matMulOp.getOperandTypes()` to extract shape information for address calculation and loop generation; for actual value operations (such as `ExtractAlignedPointerAsIndexOp`), use the original memref value, because MLIR's memref operations still require MemRefType.

Another key design decision is: `MatMulOp`'s lowering should directly generate intrinsic operations (`Mvin_IntrOp`, `Mul_Warp16_IntrOp`, `Mvout_IntrOp`), rather than generating `MvinOp`, `MvoutOp` and then waiting for them to be lowered. The reason is that in the LLVM lowering stage, the type system has already been converted, and creating high-level Buckyball operations again would cause type mismatch issues. Directly generating intrinsics avoids multiple type conversions and makes the code clearer and more efficient. Referring to the Gemmini dialect implementation, we adopted the same strategy.

## Test System

To verify the correctness of the new architecture, we created complete test cases in the `bb-tests/workloads/src/OpTest/tile/` directory. Tests are divided into two categories: staged tests and end-to-end tests.

`tile-matmul.mlir` tests the conversion from Linalg to Tile, verifying that `linalg.matmul` is correctly converted to `tile.tile_matmul`. This is the most basic type conversion test. `tile-to-buckyball.mlir` tests the conversion from Tile to Buckyball, verifying that the tiling logic is correct and that the correct number of `buckyball.bb_matmul` operations and `memref.subview` operations are generated. `buckyball-to-llvm.mlir` tests the conversion from Buckyball MatMulOp to LLVM intrinsics, verifying that the correct sequences of `buckyball.intr.bb_mvin`, `buckyball.intr.bb_mul_warp16`, and `buckyball.intr.bb_mvout` instructions are generated.

`end-to-end.mlir` is the most important test, testing the complete conversion flow: starting from `linalg.matmul`, sequentially passing through the three passes `-convert-linalg-to-tile`, `-convert-tile-to-buckyball`, `-lower-buckyball`, and finally generating LLVM intrinsics. This test ensures that each part of the entire pipeline works correctly and that there are no issues with the connections between parts.

## Pass Registration and Toolchain Integration

The two newly added passes need to be registered in multiple places. First, register the pass creation functions `registerLowerLinalgToTilePass()` and `registerLowerTileToBuckyballPass()` in `InitAll.cpp`, and also register `buddy::tile::TileDialect`. In the `buddy-opt` tool, `buddy::tile::TileDialect` needs to be added to the dialect registry so that the tool can recognize and parse tile dialect operations. In the CMake build system, the new libraries `BuddyTile`, `LowerLinalgToTilePass`, and `LowerTileToBuckyballPass` need to be added to the link dependencies, ensuring correct dependency relationships.

It is particularly worth noting that in the `configureBuckyballLegalizeForExportTarget` function in `LegalizeForLLVMExport.cpp`, we need to add `target.addLegalDialect<memref::MemRefDialect>()` and `target.addLegalDialect<arith::ArithDialect>()`, because memref and arith operations will be used during the lowering process of `MatMulOp`. If these dialects are not marked as legal, the conversion framework will attempt to lower these operations, causing type conversion conflicts.
